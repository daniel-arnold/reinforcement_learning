{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0ac8633af004fb6dddac50d247fba5c98c997cdcd9c7096e37c8d25ee3fa7bf59",
   "display_name": "Python 3.7.10 64-bit ('tf': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import pybullet_envs\n",
    "import pandas as pd"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for normalizing the observations\n",
    "class Normalizer():\n",
    "    #Welford's online algorithm\n",
    "    #implementation from: iamsuvhro\n",
    "    def __init__(self, n_inputs):\n",
    "        self.mean = np.zeros(n_inputs)\n",
    "        self.n = np.zeros(n_inputs)\n",
    "        self.sos_diff = np.zeros(n_inputs)\n",
    "        self.var = np.zeros(n_inputs)\n",
    "\n",
    "    def update_statistics(self, x):\n",
    "        self.n += 1\n",
    "        #update mean \n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean)/self.n\n",
    "        #update sum of squares differences\n",
    "        self.sos_diff += (x-last_mean)*(x-self.mean)\n",
    "        self.var = (self.sos_diff/self.n).clip(min=1e-2)\n",
    "\n",
    "    def normalize(self, u):\n",
    "        self.update_statistics(u)\n",
    "        u_no_mean = u - self.mean\n",
    "        u_std = np.sqrt(self.var)\n",
    "        return u_no_mean/u_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for ARS agent\n",
    "class ARSAgent():\n",
    "    def __init__(self):\n",
    "        self.alpha = 0.0002 #learning rate\n",
    "        self.mu = 0.003 #exploration noise\n",
    "        self.num_directions = 4 #number of random directions to consider\n",
    "        self.num_best_directions = 4 #number of best directions to consider\n",
    "        assert self.num_best_directions <= self.num_directions\n",
    "        self.max_iterations = 30 #number of iterations\n",
    "        self.max_episode_steps = 1000 #max steps in episode\n",
    "        self.env_name = 'CartPole-v1'\n",
    "        self.env = gym.make(self.env_name)\n",
    "        self.n_inputs = self.env.observation_space.shape[0]\n",
    "        self.n_outputs = 1\n",
    "        #self.seed = 1\n",
    "        #np.random.seed(self.seed)\n",
    "        self.theta = np.zeros((self.n_inputs,self.n_outputs))\n",
    "        self.normalizer = Normalizer(self.n_inputs)\n",
    "\n",
    "    def get_action(self, state, theta):\n",
    "        _u = np.dot(theta.T, state)\n",
    "        if _u > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def rollout(self, theta):\n",
    "        state = self.env.reset()\n",
    "        #rollout for episode:\n",
    "        done = False\n",
    "        sum_rewards = 0\n",
    "        k = 0\n",
    "        while not done and k<self.max_episode_steps:\n",
    "            #normalize state\n",
    "            state = self.normalizer.normalize(state)\n",
    "            #get next action\n",
    "            u = self.get_action(state, theta)\n",
    "            state, reward, done, _ = self.env.step(u)\n",
    "            #reward = max(min(reward, 1), -1)\n",
    "            sum_rewards += reward\n",
    "            k+=1\n",
    "        return sum_rewards\n",
    "\n",
    "    def random_directions(self):\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(self.num_directions)]\n",
    "\n",
    "    def random_search(self):\n",
    "        #run 1 iteration of augmented random search\n",
    "        d = self.random_directions()\n",
    "        r_pos = []\n",
    "        r_neg = []\n",
    "        for i in range(0,self.num_directions):\n",
    "            #generate random direction\n",
    "            _d = d[i]\n",
    "            #rollout in _d and -_d\n",
    "            theta_d_pos = self.theta + self.mu * _d\n",
    "            theta_d_neg = self.theta - self.mu * _d\n",
    "            #compute positive and negative rewards\n",
    "            r_pos.append(self.rollout(theta_d_pos))\n",
    "            r_neg.append(self.rollout(theta_d_neg))\n",
    "\n",
    "        #compute std for rewards\n",
    "        r_std = np.asarray(r_pos + r_neg).std()\n",
    "\n",
    "        #find indices of best b rewards\n",
    "        best_scores = [max(_r_pos, _r_neg) for k,(_r_pos,_r_neg) in enumerate(zip(r_pos, r_neg))]\n",
    "        idxs = np.asarray(best_scores).argsort()[-self.num_best_directions:]\n",
    "        #GD\n",
    "        _theta = np.zeros(self.theta.shape)\n",
    "        for idx in list(idxs):\n",
    "            _theta += self.alpha/self.num_best_directions * (r_pos[idx] - r_neg[idx])/r_std * d[idx]\n",
    "        #update theta\n",
    "        self.theta += _theta\n",
    "        #rollout with the new policy for evaluation\n",
    "        r_eval = self.rollout(self.theta)\n",
    "        return self.theta, r_eval\n",
    "\n",
    "    def train(self):\n",
    "        k=0\n",
    "        thetas = []\n",
    "        rewards = []\n",
    "        while k<self.max_iterations:\n",
    "            #run step of ARS\n",
    "            _theta, _r = self.random_search()\n",
    "            thetas.append(_theta)\n",
    "            rewards.append(_r)\n",
    "            print(\"Iteration: \", k, \" ---------- reward: \", _r)\n",
    "            k+=1\n",
    "        return thetas,rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration:  0  ---------- reward:  79.0\n",
      "Iteration:  1  ---------- reward:  42.0\n",
      "Iteration:  2  ---------- reward:  392.0\n",
      "Iteration:  3  ---------- reward:  390.0\n",
      "Iteration:  4  ---------- reward:  197.0\n",
      "Iteration:  5  ---------- reward:  339.0\n",
      "Iteration:  6  ---------- reward:  500.0\n",
      "Iteration:  7  ---------- reward:  500.0\n",
      "Iteration:  8  ---------- reward:  500.0\n",
      "Iteration:  9  ---------- reward:  356.0\n",
      "Iteration:  10  ---------- reward:  500.0\n",
      "Iteration:  11  ---------- reward:  500.0\n",
      "Iteration:  12  ---------- reward:  500.0\n",
      "Iteration:  13  ---------- reward:  500.0\n",
      "Iteration:  14  ---------- reward:  500.0\n",
      "Iteration:  15  ---------- reward:  500.0\n",
      "Iteration:  16  ---------- reward:  500.0\n",
      "Iteration:  17  ---------- reward:  500.0\n",
      "Iteration:  18  ---------- reward:  500.0\n",
      "Iteration:  19  ---------- reward:  500.0\n",
      "Iteration:  20  ---------- reward:  500.0\n",
      "Iteration:  21  ---------- reward:  500.0\n",
      "Iteration:  22  ---------- reward:  500.0\n",
      "Iteration:  23  ---------- reward:  500.0\n",
      "Iteration:  24  ---------- reward:  500.0\n",
      "Iteration:  25  ---------- reward:  500.0\n",
      "Iteration:  26  ---------- reward:  500.0\n",
      "Iteration:  27  ---------- reward:  500.0\n",
      "Iteration:  28  ---------- reward:  500.0\n",
      "Iteration:  29  ---------- reward:  500.0\n"
     ]
    }
   ],
   "source": [
    "#create ARS agent\n",
    "ars = ARSAgent()\n",
    "#train the agent\n",
    "thetas,rewards = ars.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store results in dataframe and save to csv\n",
    "rewards_dict = {'rewards':rewards}\n",
    "rewards_df = pd.DataFrame(rewards_dict)\n",
    "rewards_df.to_csv('ARS_cartpole_rewards.csv')\n",
    "\n",
    "#save last 10 weights to csv\n",
    "thetas_dict = {'theta':thetas[-10:-1]}\n",
    "thetas_df = pd.DataFrame(thetas_dict)\n",
    "thetas_df.to_csv('ARS_cartpole_thetas.csv')"
   ]
  }
 ]
}